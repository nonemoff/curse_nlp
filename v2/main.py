#!/usr/bin/env python3
"""
NLP –ö—É—Ä—Å–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞ - CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–µ—Å—É—Ä—Å–∞
"""
import argparse
import sys
from pathlib import Path

from rich.console import Console
from rich.table import Table

# Add parent dir to path
sys.path.insert(0, str(Path(__file__).parent))

import config
from core.pdf_parser import PDFParser
from core.preprocessor import Preprocessor
from core.frequency import FrequencyAnalyzer
from core.term_index import TermIndexBuilder
from core.ner import NERExtractor
from core.cache import CacheManager

console = Console()


def cmd_extract(args):
    """Extract text from PDF files"""
    parser = PDFParser(config.CORPUS_DIR, config.OUTPUT_DIR / "extracted")
    
    console.print("[bold cyan]üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF...[/bold cyan]")
    results = parser.extract_all()
    
    # Show statistics
    table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è")
    table.add_column("–§–∞–π–ª", style="cyan")
    table.add_column("–Ø–∑—ã–∫", style="green")
    table.add_column("–ó–Ω–∞–∫–æ–≤", justify="right", style="yellow")
    
    for result in results[:10]:  # Show first 10
        table.add_row(
            result['filename'],
            result['language'],
            f"{result['char_count']:,}"
        )
    
    console.print(table)
    console.print(f"\n[green]‚úì[/green] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(results)} —Ñ–∞–π–ª–æ–≤")
    
    # Save to cache
    cache = CacheManager()
    cache.save('extracted', results)


def cmd_analyze(args):
    """Perform frequency analysis"""
    console.print("[bold cyan]üìä –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑...[/bold cyan]")
    
    # Load extracted texts
    cache = CacheManager()
    extracted = cache.load('extracted')
    
    if not extracted:
        console.print("[red]–û—à–∏–±–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ extract[/red]")
        return
    
    # Preprocess
    preprocessor = Preprocessor()
    texts = [item['text'] for item in extracted]
    tokens, lemmas = preprocessor.process_texts(texts)
    
    # Analyze
    analyzer = FrequencyAnalyzer()
    results = analyzer.analyze(lemmas)
    
    # Save results
    analyzer.save_results(config.OUTPUT_DIR)
    analyzer.plot_zipf(config.OUTPUT_DIR / "graphs" / "zipf.png")
    analyzer.plot_cumulative(config.OUTPUT_DIR / "graphs" / "cumulative.png")
    
    cache.save('frequency', results)
    
    console.print(f"\n[green]‚úì[/green] M={results['M']:,}, N={results['N']:,}, K_R={results['K_R']:.2f}")


def cmd_terms(args):
    """Build terminological index"""
    console.print("[bold cyan]üìö –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —É–∫–∞–∑–∞—Ç–µ–ª—å...[/bold cyan]")
    
    cache = CacheManager()
    extracted = cache.load('extracted')
    
    if not extracted:
        console.print("[red]–û—à–∏–±–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ extract[/red]")
        return
    
    # Build index
    builder = TermIndexBuilder()
    texts = [item['text'] for item in extracted]
    results = builder.build_index(texts)
    
    # Save
    builder.save_results(config.OUTPUT_DIR)
    cache.save('terms', results)
    
    console.print(f"\n[green]‚úì[/green] –í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {results['total']}")
    console.print(f"  - –û–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–µ: {len(results['terms'])}")
    console.print(f"  - 2-—Å–ª–æ–≤–Ω—ã–µ: {len(results['bigrams'])}")
    console.print(f"  - 3-—Å–ª–æ–≤–Ω—ã–µ: {len(results['trigrams'])}")
    console.print(f"  - –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã: {len(results['abbreviations'])}")


def cmd_names(args):
    """Extract named entities"""
    console.print("[bold cyan]üë§ –ò–º–µ–Ω–Ω–æ–π —É–∫–∞–∑–∞—Ç–µ–ª—å...[/bold cyan]")
    
    cache = CacheManager()
    extracted = cache.load('extracted')
    
    if not extracted:
        console.print("[red]–û—à–∏–±–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ extract[/red]")
        return
    
    # Extract NER
    extractor = NERExtractor()
    results = extractor.extract_from_corpus(extracted)
    
    # Save
    extractor.save_results(config.OUTPUT_DIR)
    cache.save('names', results)
    
    console.print(f"\n[green]‚úì[/green] –í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {results['total']}")
    for category, entities in results['by_category'].items():
        console.print(f"  - {category}: {len(entities)}")


def cmd_all(args):
    """Run full pipeline"""
    cmd_extract(args)
    cmd_analyze(args)
    cmd_terms(args)
    cmd_names(args)
    console.print("\n[bold green]‚úì –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω![/bold green]")


def cmd_status(args):
    """Show cache status"""
    cache = CacheManager()
    status = cache.get_status()
    
    table = Table(title="–°—Ç–∞—Ç—É—Å –∫–µ—à–∞")
    table.add_column("–ú–æ–¥—É–ª—å", style="cyan")
    table.add_column("–°—Ç–∞—Ç—É—Å", style="green")
    table.add_column("–†–∞–∑–º–µ—Ä", justify="right")
    
    for module, info in status.items():
        table.add_row(
            module,
            "‚úì –ï—Å—Ç—å" if info['exists'] else "‚úó –ù–µ—Ç",
            info.get('size', '-')
        )
    
    console.print(table)


def cmd_clear(args):
    """Clear cache"""
    cache = CacheManager()
    cache.clear_all()
    console.print("[green]‚úì –ö–µ—à –æ—á–∏—â–µ–Ω[/green]")


def main():
    parser = argparse.ArgumentParser(description="NLP –ö—É—Ä—Å–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞")
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥—ã')
    
    # Commands
    subparsers.add_parser('extract', help='–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF')
    subparsers.add_parser('analyze', help='–ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑')
    subparsers.add_parser('terms', help='–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —É–∫–∞–∑–∞—Ç–µ–ª—å')
    subparsers.add_parser('names', help='–ò–º–µ–Ω–Ω–æ–π —É–∫–∞–∑–∞—Ç–µ–ª—å')
    subparsers.add_parser('all', help='–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω')
    subparsers.add_parser('status', help='–°—Ç–∞—Ç—É—Å –∫–µ—à–∞')
    subparsers.add_parser('clear', help='–û—á–∏—Å—Ç–∏—Ç—å –∫–µ—à')
    
    # Parse
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Dispatch
    commands = {
        'extract': cmd_extract,
        'analyze': cmd_analyze,
        'terms': cmd_terms,
        'names': cmd_names,
        'all': cmd_all,
        'status': cmd_status,
        'clear': cmd_clear,
    }
    
    commands[args.command](args)


if __name__ == '__main__':
    main()
